{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mugdha1503/Multipath_TCP_with_DRL/blob/main/Copy_of_RL%5E2_Scheduler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLy-80bypJfV",
        "outputId": "4a784713-6f00-489a-ebef-f614eb92ae70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1/10, Avg Reward: 5.11\n",
            "Trial 2/10, Avg Reward: 29.03\n",
            "Trial 3/10, Avg Reward: 34.20\n",
            "Trial 4/10, Avg Reward: 34.19\n",
            "Trial 5/10, Avg Reward: 34.19\n",
            "Trial 6/10, Avg Reward: 34.19\n",
            "Trial 7/10, Avg Reward: 34.19\n",
            "Trial 8/10, Avg Reward: 34.19\n",
            "Trial 9/10, Avg Reward: 34.19\n",
            "Trial 10/10, Avg Reward: 34.19\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pandas as pd\n",
        "import random\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "file_path = '/content/Dataset2.csv'\n",
        "dataset = pd.read_csv(file_path)\n",
        "\n",
        "columns_to_clean = ['P1_RTT', 'P1_CWND', 'P1_inflight', 'P2_RTT', 'P2_CWND', 'P2_inflight']\n",
        "\n",
        "# Clean columns\n",
        "def clean_column(col):\n",
        "    cleaned_col = pd.to_numeric(dataset[col], errors='coerce')\n",
        "    cleaned_col.fillna(cleaned_col.mean(), inplace=True)\n",
        "    return cleaned_col\n",
        "\n",
        "for col in columns_to_clean:\n",
        "    dataset[col] = clean_column(col)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "dataset = dataset.drop(columns=['Unnamed: 6'], errors='ignore')\n",
        "\n",
        "# Normalize columns\n",
        "def normalize(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    return (column - min_val) / (max_val - min_val)\n",
        "\n",
        "for col in columns_to_clean:\n",
        "    dataset[col] = normalize(dataset[col])\n",
        "\n",
        "# Reward calculation function\n",
        "def calculate_reward(path_1, path_2, action):\n",
        "    weights = [0.5, 0.3, 0.2]\n",
        "    score_1 = sum(w * p for w, p in zip(weights, path_1))\n",
        "    score_2 = sum(w * p for w, p in zip(weights, path_2))\n",
        "    reward = score_2 - score_1 if action == 0 else score_1 - score_2\n",
        "    return max(-100, min(reward, 100))\n",
        "\n",
        "# Define custom environment\n",
        "class NetworkEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(NetworkEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.current_step = 0\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(6,), dtype=np.float32)  # Expanded to 6 features\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "\n",
        "    def step(self, action):\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "        path_1 = [current_data['P1_RTT'], current_data['P1_CWND'], current_data['P1_inflight']]\n",
        "        path_2 = [current_data['P2_RTT'], current_data['P2_CWND'], current_data['P2_inflight']]\n",
        "        reward = calculate_reward(path_1, path_2, action)\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data)\n",
        "\n",
        "        # Update the next state based on the selected action (Path 1 or Path 2)\n",
        "        new_state = np.array(path_1 + path_2, dtype=np.float32)  # Both paths in the state\n",
        "        return new_state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        first_row = self.data.iloc[self.current_step]\n",
        "\n",
        "        # Return both Path 1 and Path 2 conditions as the starting state\n",
        "        initial_state = np.array([first_row['P1_RTT'], first_row['P1_CWND'], first_row['P1_inflight'],\n",
        "                                  first_row['P2_RTT'], first_row['P2_CWND'], first_row['P2_inflight']], dtype=np.float32)\n",
        "        return initial_state\n",
        "\n",
        "# RNN policy with GRU\n",
        "class RNNPolicy(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNPolicy, self).__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # Ensure the input is in the correct shape (seq_len, batch, input_size)\n",
        "        x = x.view(-1, 1, self.gru.input_size)\n",
        "        out, hidden = self.gru(x, hidden)\n",
        "        logits = self.fc(out.squeeze(0))\n",
        "        return logits, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, 1, self.gru.hidden_size)\n",
        "\n",
        "# Embedding inputs for RNN\n",
        "def embed_input(state, action, reward, done):\n",
        "    action_vec = np.zeros(2)\n",
        "    action_vec[action] = 1.0\n",
        "    termination_flag = 1.0 if done else 0.0\n",
        "    # Ensure the input is in the correct shape (1, input_size)\n",
        "    input_tensor = np.concatenate([state, action_vec, [reward, termination_flag]])\n",
        "    return torch.FloatTensor(input_tensor).unsqueeze(0)  # Add an extra dimension for the sequence length\n",
        "\n",
        "# GAE computation for variance reduction\n",
        "def compute_gae(rewards, values, gamma=0.99, lam=0.95):\n",
        "    returns = []\n",
        "    gae = 0\n",
        "    next_value = 0\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * next_value - values[step]\n",
        "        gae = delta + gamma * lam * gae\n",
        "        returns.insert(0, gae + values[step])\n",
        "        next_value = values[step]\n",
        "    return torch.FloatTensor(returns)\n",
        "\n",
        "# TRPO Policy Optimization (using a constraint on KL divergence)\n",
        "def trpo_step(policy, trajectories, max_kl=0.01):\n",
        "    states, actions, returns, old_log_probs = zip(*trajectories)\n",
        "\n",
        "    # Convert to tensors\n",
        "    states = torch.stack(states)\n",
        "    actions = torch.LongTensor(actions)\n",
        "    returns = torch.FloatTensor(returns)\n",
        "    old_log_probs = torch.FloatTensor(old_log_probs)\n",
        "\n",
        "    def compute_loss():\n",
        "        logits, _ = policy(states, policy.init_hidden())\n",
        "        new_log_probs = Categorical(logits=logits).log_prob(actions)\n",
        "        loss = -torch.mean((returns - returns.mean()) * new_log_probs)\n",
        "        return loss\n",
        "\n",
        "    # Backprop and gradient update\n",
        "    loss = compute_loss()\n",
        "    policy.zero_grad()\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(policy.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "# Train the meta-policy using RL²\n",
        "def train_rl2(env, policy, optimizer, num_trials, num_episodes):\n",
        "    for trial in range(num_trials):\n",
        "        total_rewards = []\n",
        "        for _ in range(num_episodes):\n",
        "            state = env.reset()\n",
        "            hidden = policy.init_hidden()\n",
        "            trajectory = []\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                state_tensor = embed_input(state, 0, 0, done)\n",
        "                logits, hidden = policy(state_tensor, hidden)\n",
        "                action = Categorical(logits=logits).sample().item()\n",
        "\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                log_prob = Categorical(logits=logits).log_prob(torch.tensor(action))\n",
        "\n",
        "                trajectory.append((state_tensor, action, reward, log_prob))\n",
        "                state = next_state\n",
        "\n",
        "            rewards = [x[2] for x in trajectory]\n",
        "            values = [0] * len(rewards)  # Placeholder, replace with critic if used\n",
        "            returns = compute_gae(rewards, values)\n",
        "\n",
        "            total_rewards.append(sum(rewards))\n",
        "            trpo_step(policy, trajectory)\n",
        "\n",
        "        print(f\"Trial {trial + 1}/{num_trials}, Avg Reward: {np.mean(total_rewards):.2f}\")\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 10\n",
        "hidden_size = 128\n",
        "output_size = 2\n",
        "num_trials = 10\n",
        "num_episodes = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize environment, policy, and optimizer\n",
        "env = NetworkEnv(dataset)\n",
        "policy = RNNPolicy(input_size, hidden_size, output_size)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the RL² meta-policy\n",
        "train_rl2(env, policy, optimizer, num_trials, num_episodes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}