{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOWXRBO-vqXV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/Dataset2.csv'\n",
        "dataset = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "MHGGmWuxvw0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_clean = ['P1_RTT', 'P1_CWND', 'P1_inflight', 'P2_RTT', 'P2_CWND', 'P2_inflight']\n",
        "\n",
        "# Function to clean each column\n",
        "def clean_column(col):\n",
        "    # Convert values to numeric, coerce errors (non-numeric entries become NaN)\n",
        "    cleaned_col = pd.to_numeric(dataset[col], errors='coerce')\n",
        "\n",
        "    # Replace NaN with the mean of the column or 0 (you can choose another value)\n",
        "    cleaned_col.fillna(cleaned_col.mean(), inplace=True)\n",
        "\n",
        "    return cleaned_col\n",
        "\n",
        "# Apply the cleaning function to all relevant columns\n",
        "for col in columns_to_clean:\n",
        "    dataset[col] = clean_column(col)\n",
        "\n",
        "# After cleaning, the dataset should only contain numeric values\n",
        "dataset = dataset.drop(columns=['Unnamed: 6'], errors='ignore')\n",
        "\n",
        "# Check the cleaned dataset\n",
        "print(dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH9JqW371iNZ",
        "outputId": "b21b9f09-8323-4935-e170-a9444e971715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   P1_RTT  P1_CWND  P1_inflight  P2_RTT  P2_CWND  P2_inflight\n",
            "0     0.0    46.72          178     0.0    46.72           25\n",
            "1     0.0    46.72         1375     0.0    46.72          178\n",
            "2     0.0    46.72         1375     0.0    46.72         1528\n",
            "3     0.0    46.72         1375     0.0    46.72         2878\n",
            "4     0.0    46.72         1375     0.0    46.72         4228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(column):\n",
        "    min_val = column.min()\n",
        "    max_val = column.max()\n",
        "    return (column - min_val) / (max_val - min_val)\n",
        "\n",
        "# Apply normalization to each column\n",
        "for col in columns_to_clean:\n",
        "    dataset[col] = normalize(dataset[col])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUi74J_zD6Tq",
        "outputId": "b1935f63-fe53-4a31-dab6-8979c192bb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_reward(path_1, path_2, action):\n",
        "    # Calculate a score for each path (example: weighted sum)\n",
        "    weights = [0.5, 0.3, 0.2]  # Example weights for RTT, CWND, and in-flight packets\n",
        "    score_1 = sum(w * p for w, p in zip(weights, path_1))\n",
        "    score_2 = sum(w * p for w, p in zip(weights, path_2))\n",
        "\n",
        "    # Determine the reward based on the chosen action and relative scores\n",
        "    if action == 0:  # Chose Path 1\n",
        "        reward = score_2 - score_1\n",
        "    else:  # Chose Path 2\n",
        "        reward = score_1 - score_2\n",
        "\n",
        "    # Ensure the reward is within a reasonable range\n",
        "    reward = max(-100, min(reward, 100))\n",
        "\n",
        "    return reward"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsK70NWNaAGh",
        "outputId": "06204b71-e3ae-4fc0-d613-a036536196cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetworkEnv(gym.Env):\n",
        "    def __init__(self, data):\n",
        "        super(NetworkEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.current_step = 0\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(3,), dtype=np.float32)\n",
        "        self.action_space = spaces.Discrete(2)  # Two possible paths: 0 for Path 1, 1 for Path 2\n",
        "\n",
        "    def step(self, action):\n",
        "        current_data = self.data.iloc[self.current_step]\n",
        "\n",
        "        # Parameters for both paths\n",
        "        path_1 = [current_data['P1_RTT'], current_data['P1_CWND'], current_data['P1_inflight']]\n",
        "        path_2 = [current_data['P2_RTT'], current_data['P2_CWND'], current_data['P2_inflight']]\n",
        "\n",
        "        # Calculate reward based on the chosen action\n",
        "        reward = calculate_reward(path_1, path_2, action)\n",
        "\n",
        "        # Increment step\n",
        "        self.current_step += 1\n",
        "        done = self.current_step >= len(self.data)\n",
        "\n",
        "        # Next state is based on the selected path parameters\n",
        "        new_state = np.array([path_1 if action == 0 else path_2], dtype=np.float32).squeeze()\n",
        "\n",
        "        return new_state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        first_row = self.data.iloc[self.current_step]\n",
        "        return np.array([first_row['P1_RTT'], first_row['P1_CWND'], first_row['P1_inflight']], dtype=np.float32)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        pass"
      ],
      "metadata": {
        "id": "uZ2gkYQjv5hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, beta, input_dims, n_actions, fc1_dims=256, fc2_dims=256):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims + n_actions, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.q = nn.Linear(fc2_dims, 1)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        q_input = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(q_input))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q = self.q(x)\n",
        "        return q"
      ],
      "metadata": {
        "id": "cKzyy2skwAwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, beta, input_dims, fc1_dims=256, fc2_dims=256):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.v = nn.Linear(fc2_dims, 1)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        v = self.v(x)\n",
        "        return v"
      ],
      "metadata": {
        "id": "JkFa--OdwDaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, alpha, input_dims, fc1_dims=256, fc2_dims=256, n_actions=2):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
        "        self.mu = nn.Linear(fc2_dims, n_actions)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        mu = torch.tanh(self.mu(x))  # Output between -1 and 1\n",
        "        return mu"
      ],
      "metadata": {
        "id": "HT59O39KwGCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *[input_shape])) # Changed input_shape to a list\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *[input_shape])) # Changed input_shape to a list\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=bool)\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = new_state\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.terminal_memory[index] = done\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        new_states = self.new_state_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, new_states, dones"
      ],
      "metadata": {
        "id": "-AhXlfvWwL3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SACAgent:\n",
        "    def __init__(self, alpha, beta, input_dims, n_actions, gamma=0.99, tau=0.005, buffer_size=100000, batch_size=64):\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = ReplayBuffer(buffer_size, input_dims, n_actions)\n",
        "\n",
        "        # Networks\n",
        "        self.actor = ActorNetwork(alpha, input_dims, n_actions=n_actions)\n",
        "        self.critic_1 = CriticNetwork(beta, input_dims, n_actions)\n",
        "        self.critic_2 = CriticNetwork(beta, input_dims, n_actions)\n",
        "        self.value = ValueNetwork(beta, input_dims)\n",
        "        self.target_value = ValueNetwork(beta, input_dims)\n",
        "\n",
        "        self.update_network_parameters(tau=1)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "      state = torch.tensor([state], dtype=torch.float32).to(self.actor.device)\n",
        "      logits = self.actor(state)\n",
        "      action_probs = torch.softmax(logits, dim=-1)\n",
        "      action_distribution = torch.distributions.Categorical(action_probs)\n",
        "      action = action_distribution.sample()\n",
        "      return action.item()\n",
        "\n",
        "\n",
        "    def store_transition(self, state, action, reward, new_state, done):\n",
        "        self.memory.store_transition(state, action, reward, new_state, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, new_states, dones = self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        states = torch.tensor(states, dtype=torch.float32).to(self.actor.device)\n",
        "        actions = torch.tensor(actions, dtype=torch.float32).to(self.actor.device)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.actor.device)\n",
        "        new_states = torch.tensor(new_states, dtype=torch.float32).to(self.actor.device)\n",
        "        dones = torch.tensor(dones, dtype=torch.bool).to(self.actor.device)\n",
        "\n",
        "        # Value loss\n",
        "        value = self.value(states).view(-1)\n",
        "        target_value = self.target_value(new_states).view(-1)\n",
        "        target_value[dones] = 0.0\n",
        "        target_value = rewards + self.gamma * target_value\n",
        "\n",
        "        value_loss = F.mse_loss(value, target_value)\n",
        "\n",
        "        self.value.optimizer.zero_grad()\n",
        "        value_loss.backward(retain_graph=True)\n",
        "        self.value.optimizer.step()\n",
        "\n",
        "        # Critic loss\n",
        "        #actions = actions.view(-1, 1)\n",
        "        critic_value_1 = self.critic_1(states, actions).view(-1)\n",
        "        critic_value_2 = self.critic_2(states, actions).view(-1)\n",
        "\n",
        "        critic_value = torch.min(critic_value_1, critic_value_2)\n",
        "\n",
        "        critic_loss = F.mse_loss(critic_value, target_value)\n",
        "\n",
        "        self.critic_1.optimizer.zero_grad()\n",
        "        self.critic_2.optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_1.optimizer.step()\n",
        "        self.critic_2.optimizer.step()\n",
        "\n",
        "        # Actor loss\n",
        "        new_actions = self.actor(states)\n",
        "        critic_value_for_new_action = self.critic_1(states, new_actions)\n",
        "\n",
        "        actor_loss = -torch.mean(critic_value_for_new_action)\n",
        "\n",
        "        self.actor.optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor.optimizer.step()\n",
        "\n",
        "        # Soft update for the target value network\n",
        "        self.update_network_parameters()\n",
        "\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "        for target_param, param in zip(self.target_value.parameters(), self.value.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n"
      ],
      "metadata": {
        "id": "GWIDEbLJwRzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sac(env, agent, episodes=100):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.choose_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.store_transition(state, action, reward, next_state, done)\n",
        "            agent.learn()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        print(f\"Episode {episode + 1}/{episodes} - Total Reward: {total_reward}\")\n"
      ],
      "metadata": {
        "id": "fFTKK1gdwWFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = NetworkEnv(dataset)\n",
        "agent = SACAgent(alpha=0.001, beta=0.001, input_dims=3, n_actions=2)\n",
        "train_sac(env, agent, episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A1S-2oQwaZf",
        "outputId": "ed5576a5-3986-4c11-8007-12e25e4b8879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100 - Total Reward: -4.98864593831923\n",
            "Episode 2/100 - Total Reward: 1.2296033150611372\n",
            "Episode 3/100 - Total Reward: -1.506571520324154\n",
            "Episode 4/100 - Total Reward: 1.798845349174171\n",
            "Episode 5/100 - Total Reward: 4.849266697580164\n",
            "Episode 6/100 - Total Reward: 3.1542001060649\n",
            "Episode 7/100 - Total Reward: -6.0731930910512535\n",
            "Episode 8/100 - Total Reward: -5.173126579283965\n",
            "Episode 9/100 - Total Reward: 0.13468801711104006\n",
            "Episode 10/100 - Total Reward: -4.346080609358543\n",
            "Episode 11/100 - Total Reward: 5.815910899912529\n",
            "Episode 12/100 - Total Reward: -7.6697290021582685\n",
            "Episode 13/100 - Total Reward: -13.147588870476138\n",
            "Episode 14/100 - Total Reward: -16.890283266799962\n",
            "Episode 15/100 - Total Reward: -3.7043036826865254\n",
            "Episode 16/100 - Total Reward: -0.2512982829439213\n",
            "Episode 17/100 - Total Reward: 15.868659455569748\n",
            "Episode 18/100 - Total Reward: 8.865308952394985\n",
            "Episode 19/100 - Total Reward: 3.9806269751394416\n",
            "Episode 20/100 - Total Reward: 15.856106203238149\n",
            "Episode 21/100 - Total Reward: 7.288451410876132\n",
            "Episode 22/100 - Total Reward: 12.9209382148658\n",
            "Episode 23/100 - Total Reward: 15.635509418554307\n",
            "Episode 24/100 - Total Reward: 16.352165747861097\n",
            "Episode 25/100 - Total Reward: 24.569727879429827\n",
            "Episode 26/100 - Total Reward: 24.92301409220383\n",
            "Episode 27/100 - Total Reward: 15.486549477026674\n",
            "Episode 28/100 - Total Reward: 25.24383539430474\n",
            "Episode 29/100 - Total Reward: 12.83879214897076\n",
            "Episode 30/100 - Total Reward: 26.75659580239458\n",
            "Episode 31/100 - Total Reward: 22.211123111827945\n",
            "Episode 32/100 - Total Reward: 18.40448096396991\n",
            "Episode 33/100 - Total Reward: 7.141892609558527\n",
            "Episode 34/100 - Total Reward: -3.9892433787239994\n",
            "Episode 35/100 - Total Reward: 3.4632935605439843\n",
            "Episode 36/100 - Total Reward: -3.5545537951125756\n",
            "Episode 37/100 - Total Reward: 2.889385309776369\n",
            "Episode 38/100 - Total Reward: -4.66894331295487\n",
            "Episode 39/100 - Total Reward: -4.567118553534344\n",
            "Episode 40/100 - Total Reward: 9.594125469966404\n",
            "Episode 41/100 - Total Reward: 2.2007327234565714\n",
            "Episode 42/100 - Total Reward: -4.062101039153189\n",
            "Episode 43/100 - Total Reward: 4.9320965608172855\n",
            "Episode 44/100 - Total Reward: 3.6571834837912873\n",
            "Episode 45/100 - Total Reward: -3.3098722407210643\n",
            "Episode 46/100 - Total Reward: -3.614132362181098\n",
            "Episode 47/100 - Total Reward: -1.672160678956431\n",
            "Episode 48/100 - Total Reward: 6.683357960030621\n",
            "Episode 49/100 - Total Reward: -3.950375271950725\n",
            "Episode 50/100 - Total Reward: -4.090001706615853\n",
            "Episode 51/100 - Total Reward: -3.4403618513211582\n",
            "Episode 52/100 - Total Reward: -2.148077846397569\n",
            "Episode 53/100 - Total Reward: -5.326855090228218\n",
            "Episode 54/100 - Total Reward: -1.3200686812897948\n",
            "Episode 55/100 - Total Reward: -2.900808325320624\n",
            "Episode 56/100 - Total Reward: 5.1938574317519794\n",
            "Episode 57/100 - Total Reward: 3.8560518362604657\n",
            "Episode 58/100 - Total Reward: 7.288259460202935\n",
            "Episode 59/100 - Total Reward: 4.69692193436577\n",
            "Episode 60/100 - Total Reward: 0.27992771037694153\n",
            "Episode 61/100 - Total Reward: -4.090040652225517\n",
            "Episode 62/100 - Total Reward: -10.906906372569587\n",
            "Episode 63/100 - Total Reward: -4.014671102327135\n",
            "Episode 64/100 - Total Reward: 7.71119953046712\n",
            "Episode 65/100 - Total Reward: -2.754183662349582\n",
            "Episode 66/100 - Total Reward: -0.9576430583457967\n",
            "Episode 67/100 - Total Reward: -4.305247201710678\n",
            "Episode 68/100 - Total Reward: 3.27494821688251\n",
            "Episode 69/100 - Total Reward: 5.894625483420498\n",
            "Episode 70/100 - Total Reward: 3.9851564435987927\n",
            "Episode 71/100 - Total Reward: -5.588029272133792\n",
            "Episode 72/100 - Total Reward: -11.270734018094593\n",
            "Episode 73/100 - Total Reward: 4.783722030911468\n",
            "Episode 74/100 - Total Reward: -6.430095904750232\n",
            "Episode 75/100 - Total Reward: -1.6693243681372336\n",
            "Episode 76/100 - Total Reward: -10.72021454775615\n",
            "Episode 77/100 - Total Reward: -0.3979327352125268\n",
            "Episode 78/100 - Total Reward: -10.436979613319473\n",
            "Episode 79/100 - Total Reward: -4.3618916605662275\n",
            "Episode 80/100 - Total Reward: -5.052085558723906\n",
            "Episode 81/100 - Total Reward: -11.676471877930467\n",
            "Episode 82/100 - Total Reward: -1.5263147063005307\n",
            "Episode 83/100 - Total Reward: -3.1680632431889815\n",
            "Episode 84/100 - Total Reward: -11.19467769500081\n",
            "Episode 85/100 - Total Reward: -9.70945381792533\n",
            "Episode 86/100 - Total Reward: -8.093516938606946\n",
            "Episode 87/100 - Total Reward: -10.084816577230509\n",
            "Episode 88/100 - Total Reward: -6.798495873608637\n",
            "Episode 89/100 - Total Reward: -7.2554332753396\n",
            "Episode 90/100 - Total Reward: -6.792141910140343\n",
            "Episode 91/100 - Total Reward: -6.931325814431466\n",
            "Episode 92/100 - Total Reward: -3.083562199391413\n",
            "Episode 93/100 - Total Reward: -4.184699420832638\n",
            "Episode 94/100 - Total Reward: 1.4402083521213291\n",
            "Episode 95/100 - Total Reward: -6.760240617409574\n",
            "Episode 96/100 - Total Reward: -12.59281349299711\n",
            "Episode 97/100 - Total Reward: -3.6739738640172224\n",
            "Episode 98/100 - Total Reward: -12.93151299106447\n",
            "Episode 99/100 - Total Reward: -4.446502081359833\n",
            "Episode 100/100 - Total Reward: 0.009070544506741496\n"
          ]
        }
      ]
    }
  ]
}